{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc98472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after cleaning: (7636, 8)\n",
      "Outliers capped at: 203.69\n",
      "Pollution statistics:\n",
      "Mean: 21.33\n",
      "Median: 13.20\n",
      "Max: 204.01\n",
      "Std: 28.08\n",
      "\n",
      "Total features created: 49\n",
      "Feature categories:\n",
      "- Base features: 6\n",
      "- Cyclical features: 8\n",
      "- Categorical features: 10\n",
      "- Geographic features: 6\n",
      "- Interaction features: 5\n",
      "- Lag features: 14\n",
      "\n",
      "Dataset split:\n",
      "Training: 5344 samples (70.0%)\n",
      "Validation: 764 samples (10.0%)\n",
      "Test: 1528 samples (20.0%)\n",
      "\n",
      "Feature selection: 40 out of 49 features selected\n",
      "\n",
      "==================================================\n",
      "Training Enhanced LightGBM...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[340]\ttraining's rmse: 2.12566\tvalid_1's rmse: 2.64381\n",
      "\n",
      "Enhanced LightGBM Performance:\n",
      "RMSE: 2.6438\n",
      "MAE: 0.4616\n",
      "R²: 0.9898\n",
      "\n",
      "==================================================\n",
      "Training Enhanced Random Forest...\n",
      "\n",
      "Enhanced Random Forest Performance:\n",
      "RMSE: 3.5438\n",
      "MAE: 0.8289\n",
      "R²: 0.9817\n",
      "\n",
      "==================================================\n",
      "Training Enhanced Huber Regressor...\n",
      "\n",
      "Enhanced Huber Regressor Performance:\n",
      "RMSE: 2.5442\n",
      "MAE: 0.3725\n",
      "R²: 0.9906\n",
      "\n",
      "==================================================\n",
      "Creating Advanced Ensemble...\n",
      "\n",
      "Simple Ensemble Performance:\n",
      "RMSE: 2.7173\n",
      "MAE: 0.5110\n",
      "R²: 0.9893\n",
      "\n",
      "Optimized model weights:\n",
      "lgb: 0.334\n",
      "rf: 0.332\n",
      "huber: 0.334\n",
      "\n",
      "Optimized Weighted Ensemble Performance:\n",
      "RMSE: 2.7160\n",
      "MAE: 0.5105\n",
      "R²: 0.9893\n",
      "\n",
      "==================================================\n",
      "Performing Cross-Validation...\n",
      "\n",
      "Cross-validation RMSE scores:\n",
      "LightGBM: 3.6978\n",
      "RandomForest: 4.3858\n",
      "Huber: 3.9830\n",
      "\n",
      "============================================================\n",
      "FINAL TEST SET EVALUATION\n",
      "============================================================\n",
      "\n",
      "LightGBM Performance:\n",
      "RMSE: 4.0882\n",
      "MAE: 0.6512\n",
      "R²: 0.9816\n",
      "\n",
      "Random Forest Performance:\n",
      "RMSE: 5.0107\n",
      "MAE: 1.1060\n",
      "R²: 0.9724\n",
      "\n",
      "Huber Regressor Performance:\n",
      "RMSE: 3.7348\n",
      "MAE: 0.5336\n",
      "R²: 0.9847\n",
      "\n",
      "Simple Ensemble Performance:\n",
      "RMSE: 4.0998\n",
      "MAE: 0.7208\n",
      "R²: 0.9815\n",
      "\n",
      "Weighted Ensemble Performance:\n",
      "RMSE: 4.0984\n",
      "MAE: 0.7201\n",
      "R²: 0.9815\n",
      "\n",
      "Best performing model: Huber Regressor\n",
      "Best RMSE: 3.7348\n",
      "\n",
      "==================================================\n",
      "Generating enhanced submission predictions...\n",
      "Test data shape: (2739, 7)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['pollution_lag_1h', 'pollution_lag_2h', 'pollution_lag_3h', 'pollution_lag_6h', 'pollution_lag_12h', 'pollution_lag_24h', 'pollution_rolling_mean_3h', 'pollution_rolling_std_3h', 'pollution_rolling_mean_6h', 'pollution_rolling_std_6h', 'pollution_rolling_mean_12h', 'pollution_rolling_std_12h', 'pollution_rolling_mean_24h', 'pollution_rolling_std_24h'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 432\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Apply same preprocessing (without lag features for test data)\u001b[39;00m\n\u001b[0;32m    431\u001b[0m test_enhanced \u001b[38;5;241m=\u001b[39m create_enhanced_features(test_df, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 432\u001b[0m X_submission \u001b[38;5;241m=\u001b[39m test_enhanced[all_features]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Apply feature selection and scaling\u001b[39;00m\n\u001b[0;32m    435\u001b[0m X_submission_selected \u001b[38;5;241m=\u001b[39m feature_selector\u001b[38;5;241m.\u001b[39mtransform(X_submission)\n",
      "File \u001b[1;32mc:\\Users\\damod\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\damod\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\damod\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['pollution_lag_1h', 'pollution_lag_2h', 'pollution_lag_3h', 'pollution_lag_6h', 'pollution_lag_12h', 'pollution_lag_24h', 'pollution_rolling_mean_3h', 'pollution_rolling_std_3h', 'pollution_rolling_mean_6h', 'pollution_rolling_std_6h', 'pollution_rolling_mean_12h', 'pollution_rolling_std_12h', 'pollution_rolling_mean_24h', 'pollution_rolling_std_24h'] not in index\""
     ]
    }
   ],
   "source": [
    "# Enhanced Pollution Prediction Model\n",
    "# =====================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from scipy.stats import boxcox\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load and clean data\n",
    "df = pd.read_csv('d:/competition/air pollution/phase 1/train.csv')\n",
    "\n",
    "# =====================================\n",
    "# 1. IMPROVED DATA CLEANING & OUTLIER HANDLING\n",
    "# =====================================\n",
    "\n",
    "def advanced_data_cleaning(df):\n",
    "    \"\"\"Advanced data cleaning with outlier detection\"\"\"\n",
    "    df_clean = df.dropna()\n",
    "    \n",
    "    # Detect and handle outliers using IQR method\n",
    "    Q1 = df_clean['pollution_value'].quantile(0.25)\n",
    "    Q3 = df_clean['pollution_value'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Cap extreme outliers instead of removing them\n",
    "    df_clean['pollution_value'] = np.clip(df_clean['pollution_value'], \n",
    "                                         lower_bound, \n",
    "                                         np.percentile(df_clean['pollution_value'], 99))\n",
    "    \n",
    "    print(f\"Dataset shape after cleaning: {df_clean.shape}\")\n",
    "    print(f\"Outliers capped at: {np.percentile(df_clean['pollution_value'], 99):.2f}\")\n",
    "    print(f\"Pollution statistics:\")\n",
    "    print(f\"Mean: {df_clean['pollution_value'].mean():.2f}\")\n",
    "    print(f\"Median: {df_clean['pollution_value'].median():.2f}\")\n",
    "    print(f\"Max: {df_clean['pollution_value'].max():.2f}\")\n",
    "    print(f\"Std: {df_clean['pollution_value'].std():.2f}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "df_clean = advanced_data_cleaning(df)\n",
    "\n",
    "# =====================================\n",
    "# 2. ENHANCED FEATURE ENGINEERING WITH LAG FEATURES\n",
    "# =====================================\n",
    "\n",
    "def create_enhanced_features(df, is_training=True):\n",
    "    \"\"\"Create comprehensive feature set with lag features\"\"\"\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # Sort by location and time for proper lag calculation\n",
    "    if 'pollution_value' in df_enhanced.columns:\n",
    "        df_enhanced = df_enhanced.sort_values(['latitude', 'longitude', 'day_of_year', 'hour'])\n",
    "    \n",
    "    # Cyclical time features (improved encoding)\n",
    "    df_enhanced['hour_sin'] = np.sin(2 * np.pi * df_enhanced['hour'] / 24)\n",
    "    df_enhanced['hour_cos'] = np.cos(2 * np.pi * df_enhanced['hour'] / 24)\n",
    "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced['month'] / 12)\n",
    "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced['month'] / 12)\n",
    "    df_enhanced['day_year_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_year'] / 365)\n",
    "    df_enhanced['day_year_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_year'] / 365)\n",
    "    df_enhanced['day_week_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
    "    df_enhanced['day_week_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
    "    \n",
    "    # Enhanced categorical time features\n",
    "    df_enhanced['is_weekend'] = (df_enhanced['day_of_week'] >= 5).astype(int)\n",
    "    df_enhanced['is_rush_hour_morning'] = df_enhanced['hour'].isin([7, 8, 9]).astype(int)\n",
    "    df_enhanced['is_rush_hour_evening'] = df_enhanced['hour'].isin([17, 18, 19]).astype(int)\n",
    "    df_enhanced['is_rush_hour'] = (df_enhanced['is_rush_hour_morning'] | df_enhanced['is_rush_hour_evening']).astype(int)\n",
    "    df_enhanced['is_night'] = df_enhanced['hour'].isin([22, 23, 0, 1, 2, 3, 4, 5]).astype(int)\n",
    "    df_enhanced['is_business_hours'] = df_enhanced['hour'].isin([9, 10, 11, 12, 13, 14, 15, 16]).astype(int)\n",
    "    \n",
    "    # Seasonal features\n",
    "    df_enhanced['is_summer'] = df_enhanced['month'].isin([6, 7, 8]).astype(int)\n",
    "    df_enhanced['is_winter'] = df_enhanced['month'].isin([12, 1, 2]).astype(int)\n",
    "    df_enhanced['is_spring'] = df_enhanced['month'].isin([3, 4, 5]).astype(int)\n",
    "    df_enhanced['is_fall'] = df_enhanced['month'].isin([9, 10, 11]).astype(int)\n",
    "    \n",
    "    # Enhanced geographic features\n",
    "    df_enhanced['lat_lon_interaction'] = df_enhanced['latitude'] * df_enhanced['longitude']\n",
    "    df_enhanced['lat_squared'] = df_enhanced['latitude'] ** 2\n",
    "    df_enhanced['lon_squared'] = df_enhanced['longitude'] ** 2\n",
    "    \n",
    "    # Distance from geographic center\n",
    "    centroid_lat = df_enhanced['latitude'].mean()\n",
    "    centroid_lon = df_enhanced['longitude'].mean()\n",
    "    df_enhanced['distance_from_center'] = np.sqrt(\n",
    "        (df_enhanced['latitude'] - centroid_lat)**2 + \n",
    "        (df_enhanced['longitude'] - centroid_lon)**2\n",
    "    )\n",
    "    \n",
    "    # Geographic clustering (more clusters for better granularity)\n",
    "    coords = df_enhanced[['latitude', 'longitude']].values\n",
    "    kmeans = KMeans(n_clusters=25, random_state=42, n_init=10)\n",
    "    df_enhanced['location_cluster'] = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Density-based features\n",
    "    df_enhanced['location_density'] = df_enhanced.groupby(['latitude', 'longitude'])['latitude'].transform('count')\n",
    "    \n",
    "    # Enhanced interaction features\n",
    "    df_enhanced['hour_month_interaction'] = df_enhanced['hour'] * df_enhanced['month']\n",
    "    df_enhanced['weekend_hour'] = df_enhanced['is_weekend'] * df_enhanced['hour']\n",
    "    df_enhanced['season_hour'] = df_enhanced['is_summer'] * df_enhanced['hour']\n",
    "    df_enhanced['cluster_hour'] = df_enhanced['location_cluster'] * df_enhanced['hour']\n",
    "    df_enhanced['rush_hour_cluster'] = df_enhanced['is_rush_hour'] * df_enhanced['location_cluster']\n",
    "    \n",
    "    # Lag features (only for training data)\n",
    "    if is_training and 'pollution_value' in df_enhanced.columns:\n",
    "        # Create time-based grouping key\n",
    "        df_enhanced['location_key'] = df_enhanced['latitude'].astype(str) + '_' + df_enhanced['longitude'].astype(str)\n",
    "        \n",
    "        # Sort by location and time\n",
    "        df_enhanced = df_enhanced.sort_values(['location_key', 'day_of_year', 'hour'])\n",
    "        \n",
    "        # Create lag features\n",
    "        for lag in [1, 2, 3, 6, 12, 24]:  # 1-3 hours, 6 hours, 12 hours, 24 hours\n",
    "            df_enhanced[f'pollution_lag_{lag}h'] = df_enhanced.groupby('location_key')['pollution_value'].shift(lag)\n",
    "        \n",
    "        # Rolling statistics\n",
    "        for window in [3, 6, 12, 24]:\n",
    "            df_enhanced[f'pollution_rolling_mean_{window}h'] = df_enhanced.groupby('location_key')['pollution_value'].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)\n",
    "            df_enhanced[f'pollution_rolling_std_{window}h'] = df_enhanced.groupby('location_key')['pollution_value'].rolling(window=window, min_periods=1).std().reset_index(0, drop=True)\n",
    "        \n",
    "        # Fill NaN values in lag features with median\n",
    "        lag_cols = [col for col in df_enhanced.columns if 'lag' in col or 'rolling' in col]\n",
    "        for col in lag_cols:\n",
    "            df_enhanced[col] = df_enhanced[col].fillna(df_enhanced[col].median())\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# Create enhanced features\n",
    "df_enhanced = create_enhanced_features(df_clean, is_training=True)\n",
    "\n",
    "# Define feature columns\n",
    "base_features = ['latitude', 'longitude', 'day_of_year', 'day_of_week', 'hour', 'month']\n",
    "cyclical_features = ['hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'day_year_sin', 'day_year_cos', 'day_week_sin', 'day_week_cos']\n",
    "categorical_features = ['is_weekend', 'is_rush_hour_morning', 'is_rush_hour_evening', 'is_rush_hour', 'is_night', 'is_business_hours', 'is_summer', 'is_winter', 'is_spring', 'is_fall']\n",
    "geographic_features = ['lat_lon_interaction', 'lat_squared', 'lon_squared', 'distance_from_center', 'location_cluster', 'location_density']\n",
    "interaction_features = ['hour_month_interaction', 'weekend_hour', 'season_hour', 'cluster_hour', 'rush_hour_cluster']\n",
    "\n",
    "# Lag features\n",
    "lag_features = [col for col in df_enhanced.columns if 'lag' in col or 'rolling' in col]\n",
    "\n",
    "all_features = base_features + cyclical_features + categorical_features + geographic_features + interaction_features + lag_features\n",
    "\n",
    "print(f\"\\nTotal features created: {len(all_features)}\")\n",
    "print(f\"Feature categories:\")\n",
    "print(f\"- Base features: {len(base_features)}\")\n",
    "print(f\"- Cyclical features: {len(cyclical_features)}\")\n",
    "print(f\"- Categorical features: {len(categorical_features)}\")\n",
    "print(f\"- Geographic features: {len(geographic_features)}\")\n",
    "print(f\"- Interaction features: {len(interaction_features)}\")\n",
    "print(f\"- Lag features: {len(lag_features)}\")\n",
    "\n",
    "# =====================================\n",
    "# 3. FEATURE SELECTION\n",
    "# =====================================\n",
    "\n",
    "def select_best_features(X, y, k=50):\n",
    "    \"\"\"Select k best features using statistical tests\"\"\"\n",
    "    selector = SelectKBest(score_func=f_regression, k=min(k, X.shape[1]))\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    selected_features = [all_features[i] for i in selector.get_support(indices=True)]\n",
    "    \n",
    "    print(f\"\\nFeature selection: {len(selected_features)} out of {len(all_features)} features selected\")\n",
    "    return X_selected, selected_features, selector\n",
    "\n",
    "# =====================================\n",
    "# 4. IMPROVED TRAIN/TEST SPLIT WITH CROSS-VALIDATION\n",
    "# =====================================\n",
    "\n",
    "# Reset index and prepare data\n",
    "df_enhanced = df_enhanced.reset_index(drop=True)\n",
    "X = df_enhanced[all_features].fillna(0)  # Fill any remaining NaN with 0\n",
    "y = df_enhanced['pollution_value']\n",
    "\n",
    "# Use time-based split for better validation\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.125, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset split:\")\n",
    "print(f\"Training: {len(X_train)} samples ({len(X_train)/len(df_enhanced)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(X_val)} samples ({len(X_val)/len(df_enhanced)*100:.1f}%)\")\n",
    "print(f\"Test: {len(X_test)} samples ({len(X_test)/len(df_enhanced)*100:.1f}%)\")\n",
    "\n",
    "# Feature selection on training data\n",
    "X_train_selected, selected_features, feature_selector = select_best_features(X_train, y_train, k=40)\n",
    "X_val_selected = feature_selector.transform(X_val)\n",
    "X_test_selected = feature_selector.transform(X_test)\n",
    "\n",
    "# =====================================\n",
    "# 5. ADVANCED SCALING AND PREPROCESSING\n",
    "# =====================================\n",
    "\n",
    "# Use RobustScaler for better outlier handling\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "X_val_scaled = scaler.transform(X_val_selected)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# =====================================\n",
    "# 6. ENHANCED MODEL TRAINING WITH HYPERPARAMETER TUNING\n",
    "# =====================================\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    \n",
    "    return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "models = {}\n",
    "predictions = {}\n",
    "\n",
    "# 1. Enhanced LightGBM with better hyperparameters\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Enhanced LightGBM...\")\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 50,\n",
    "    'learning_rate': 0.03,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 20,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(X_train_selected, label=y_train)\n",
    "val_data = lgb.Dataset(X_val_selected, label=y_val, reference=train_data)\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    lgb_params,\n",
    "    train_data,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "models['lgb'] = lgb_model\n",
    "predictions['lgb_val'] = lgb_model.predict(X_val_selected)\n",
    "predictions['lgb_test'] = lgb_model.predict(X_test_selected)\n",
    "\n",
    "evaluate_model(y_val, predictions['lgb_val'], \"Enhanced LightGBM\")\n",
    "\n",
    "# 2. Enhanced Random Forest\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Enhanced Random Forest...\")\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=20,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "models['rf'] = rf_model\n",
    "predictions['rf_val'] = rf_model.predict(X_val_scaled)\n",
    "predictions['rf_test'] = rf_model.predict(X_test_scaled)\n",
    "\n",
    "evaluate_model(y_val, predictions['rf_val'], \"Enhanced Random Forest\")\n",
    "\n",
    "# 3. Enhanced Huber Regressor\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Enhanced Huber Regressor...\")\n",
    "\n",
    "huber_model = HuberRegressor(epsilon=1.2, alpha=0.001, max_iter=2000)\n",
    "huber_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "models['huber'] = huber_model\n",
    "predictions['huber_val'] = huber_model.predict(X_val_scaled)\n",
    "predictions['huber_test'] = huber_model.predict(X_test_scaled)\n",
    "\n",
    "evaluate_model(y_val, predictions['huber_val'], \"Enhanced Huber Regressor\")\n",
    "\n",
    "# =====================================\n",
    "# 7. ADVANCED ENSEMBLE WITH STACKING\n",
    "# =====================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Creating Advanced Ensemble...\")\n",
    "\n",
    "# Simple averaging ensemble\n",
    "ensemble_val = (predictions['lgb_val'] + predictions['rf_val'] + predictions['huber_val']) / 3\n",
    "ensemble_test = (predictions['lgb_test'] + predictions['rf_test'] + predictions['huber_test']) / 3\n",
    "\n",
    "evaluate_model(y_val, ensemble_val, \"Simple Ensemble\")\n",
    "\n",
    "# Optimized weighted ensemble\n",
    "val_scores = {\n",
    "    'lgb': mean_squared_error(y_val, predictions['lgb_val']),\n",
    "    'rf': mean_squared_error(y_val, predictions['rf_val']),\n",
    "    'huber': mean_squared_error(y_val, predictions['huber_val'])\n",
    "}\n",
    "\n",
    "# Calculate weights using exponential of negative MSE\n",
    "weights = {}\n",
    "neg_mse_exp = {k: np.exp(-v/1000) for k, v in val_scores.items()}  # Scale MSE\n",
    "total_weight = sum(neg_mse_exp.values())\n",
    "for model in val_scores:\n",
    "    weights[model] = neg_mse_exp[model] / total_weight\n",
    "\n",
    "print(f\"\\nOptimized model weights:\")\n",
    "for model, weight in weights.items():\n",
    "    print(f\"{model}: {weight:.3f}\")\n",
    "\n",
    "# Weighted ensemble\n",
    "weighted_ensemble_val = (\n",
    "    weights['lgb'] * predictions['lgb_val'] + \n",
    "    weights['rf'] * predictions['rf_val'] + \n",
    "    weights['huber'] * predictions['huber_val']\n",
    ")\n",
    "\n",
    "weighted_ensemble_test = (\n",
    "    weights['lgb'] * predictions['lgb_test'] + \n",
    "    weights['rf'] * predictions['rf_test'] + \n",
    "    weights['huber'] * predictions['huber_test']\n",
    ")\n",
    "\n",
    "evaluate_model(y_val, weighted_ensemble_val, \"Optimized Weighted Ensemble\")\n",
    "\n",
    "# =====================================\n",
    "# 8. CROSS-VALIDATION FOR ROBUST EVALUATION\n",
    "# =====================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Performing Cross-Validation...\")\n",
    "\n",
    "# Use TimeSeriesSplit for better temporal validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_scores = {}\n",
    "\n",
    "for name, model in [('LightGBM', lgb_model), ('RandomForest', rf_model), ('Huber', huber_model)]:\n",
    "    if name == 'LightGBM':\n",
    "        # For LightGBM, we need to retrain for each fold\n",
    "        scores = []\n",
    "        for train_idx, val_idx in tscv.split(X_train_selected):\n",
    "            X_fold_train, X_fold_val = X_train_selected[train_idx], X_train_selected[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            \n",
    "            fold_train_data = lgb.Dataset(X_fold_train, label=y_fold_train)\n",
    "            fold_model = lgb.train(lgb_params, fold_train_data, num_boost_round=500)\n",
    "            fold_pred = fold_model.predict(X_fold_val)\n",
    "            scores.append(np.sqrt(mean_squared_error(y_fold_val, fold_pred)))\n",
    "        cv_scores[name] = np.mean(scores)\n",
    "    else:\n",
    "        # For sklearn models\n",
    "        if name == 'RandomForest':\n",
    "            X_data, y_data = X_train_scaled, y_train\n",
    "        else:\n",
    "            X_data, y_data = X_train_scaled, y_train\n",
    "        \n",
    "        scores = cross_val_score(model, X_data, y_data, cv=tscv, scoring='neg_root_mean_squared_error')\n",
    "        cv_scores[name] = -scores.mean()\n",
    "\n",
    "print(\"\\nCross-validation RMSE scores:\")\n",
    "for name, score in cv_scores.items():\n",
    "    print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "# =====================================\n",
    "# 9. FINAL MODEL SELECTION AND EVALUATION\n",
    "# =====================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_models = {\n",
    "    'LightGBM': predictions['lgb_test'],\n",
    "    'Random Forest': predictions['rf_test'],\n",
    "    'Huber Regressor': predictions['huber_test'],\n",
    "    'Simple Ensemble': ensemble_test,\n",
    "    'Weighted Ensemble': weighted_ensemble_test\n",
    "}\n",
    "\n",
    "test_results = {}\n",
    "for name, pred in final_models.items():\n",
    "    test_results[name] = evaluate_model(y_test, pred, name)\n",
    "\n",
    "# Find best model\n",
    "best_model = min(test_results.keys(), key=lambda x: test_results[x]['rmse'])\n",
    "print(f\"\\nBest performing model: {best_model}\")\n",
    "print(f\"Best RMSE: {test_results[best_model]['rmse']:.4f}\")\n",
    "\n",
    "# =====================================\n",
    "# 10. ENHANCED SUBMISSION GENERATION\n",
    "# =====================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Generating enhanced submission predictions...\")\n",
    "\n",
    "# Load and preprocess test data\n",
    "test_df = pd.read_csv('d:/competition/air pollution/phase 1/test.csv')\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Apply same preprocessing (without lag features for test data)\n",
    "test_enhanced = create_enhanced_features(test_df, is_training=False)\n",
    "X_submission = test_enhanced[all_features].fillna(0)\n",
    "\n",
    "# Apply feature selection and scaling\n",
    "X_submission_selected = feature_selector.transform(X_submission)\n",
    "X_submission_scaled = scaler.transform(X_submission_selected)\n",
    "\n",
    "# Generate predictions using the best ensemble\n",
    "submission_predictions = (\n",
    "    weights['lgb'] * lgb_model.predict(X_submission_selected) + \n",
    "    weights['rf'] * rf_model.predict(X_submission_scaled) + \n",
    "    weights['huber'] * huber_model.predict(X_submission_scaled)\n",
    ")\n",
    "\n",
    "# Post-processing: ensure no negative predictions and apply reasonable bounds\n",
    "submission_predictions = np.clip(submission_predictions, 0, np.percentile(y_train, 99.5))\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': range(len(submission_predictions)),\n",
    "    'pollution_value': submission_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('d:/competition/air pollution/enhanced_submission.csv', index=False)\n",
    "print(\"Enhanced submission file saved!\")\n",
    "\n",
    "print(f\"\\nSubmission statistics:\")\n",
    "print(f\"Mean prediction: {submission_predictions.mean():.2f}\")\n",
    "print(f\"Median prediction: {np.median(submission_predictions):.2f}\")\n",
    "print(f\"Min prediction: {submission_predictions.min():.2f}\")\n",
    "print(f\"Max prediction: {submission_predictions.max():.2f}\")\n",
    "\n",
    "# =====================================\n",
    "# 11. ENHANCED VISUALIZATIONS\n",
    "# =====================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Creating enhanced visualizations...\")\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Actual vs Predicted scatter plot\n",
    "plt.subplot(3, 4, 1)\n",
    "plt.scatter(y_test, weighted_ensemble_test, alpha=0.6, s=20)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted (Best Ensemble)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residuals plot\n",
    "plt.subplot(3, 4, 2)\n",
    "residuals = y_test - weighted_ensemble_test\n",
    "plt.scatter(weighted_ensemble_test, residuals, alpha=0.6, s=20)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Feature importance\n",
    "plt.subplot(3, 4, 3)\n",
    "if len(selected_features) > 0:\n",
    "    lgb_importance = lgb_model.feature_importance(importance_type='gain')\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': lgb_importance\n",
    "    }).sort_values('importance', ascending=False).head(10)\n",
    "    \n",
    "    plt.barh(range(len(feature_importance_df)), feature_importance_df['importance'])\n",
    "    plt.yticks(range(len(feature_importance_df)), feature_importance_df['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 10 Feature Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "# 4. Model comparison\n",
    "plt.subplot(3, 4, 4)\n",
    "model_names = list(test_results.keys())\n",
    "rmse_scores = [test_results[name]['rmse'] for name in model_names]\n",
    "bars = plt.bar(model_names, rmse_scores, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'lightpink'])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight best model\n",
    "best_idx = rmse_scores.index(min(rmse_scores))\n",
    "bars[best_idx].set_color('darkgreen')\n",
    "\n",
    "# 5. Prediction distribution\n",
    "plt.subplot(3, 4, 5)\n",
    "plt.hist(submission_predictions, bins=50, alpha=0.7, label='Predictions', color='skyblue')\n",
    "plt.hist(y_train, bins=50, alpha=0.7, label='Training Data', color='orange')\n",
    "plt.xlabel('Pollution Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction Distribution vs Training Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Hourly pattern\n",
    "plt.subplot(3, 4, 6)\n",
    "hourly_avg = df_enhanced.groupby('hour')['pollution_value'].mean()\n",
    "plt.plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2, markersize=4)\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Average Pollution')\n",
    "plt.title('Average Pollution by Hour')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Cross-validation scores\n",
    "plt.subplot(3, 4, 7)\n",
    "cv_names = list(cv_scores.keys())\n",
    "cv_rmse = list(cv_scores.values())\n",
    "plt.bar(cv_names, cv_rmse, color='lightsteelblue')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('CV RMSE')\n",
    "plt.title('Cross-Validation Performance')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Residuals histogram\n",
    "plt.subplot(3, 4, 8)\n",
    "plt.hist(residuals, bins=50, alpha=0.7, color='lightcoral')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residuals Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Location cluster analysis\n",
    "plt.subplot(3, 4, 9)\n",
    "cluster_pollution = df_enhanced.groupby('location_cluster')['pollution_value'].mean().sort_values(ascending=False)\n",
    "plt.bar(range(len(cluster_pollution)), cluster_pollution.values, color='lightgreen')\n",
    "plt.xlabel('Location Cluster')\n",
    "plt.ylabel('Average Pollution')\n",
    "plt.title('Average Pollution by Location Cluster')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 10. Monthly pattern\n",
    "plt.subplot(3, 4, 10)\n",
    "monthly_avg = df_enhanced.groupby('month')['pollution_value'].mean()\n",
    "plt.plot(monthly_avg.index, monthly_avg.values, marker='o', linewidth=2, markersize=4, color='purple')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Pollution')\n",
    "plt.title('Average Pollution by Month')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 11. Weekend vs Weekday\n",
    "plt.subplot(3, 4, 11)\n",
    "weekend_avg = df_enhanced.groupby('is_weekend')['pollution_value'].mean()\n",
    "plt.bar(['Weekday', 'Weekend'], weekend_avg.values, color=['lightblue', 'lightpink'])\n",
    "plt.ylabel('Average Pollution')\n",
    "plt.title('Average Pollution: Weekday vs Weekend')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 12. R² comparison\n",
    "plt.subplot(3, 4, 12)\n",
    "r2_scores = [test_results[name]['r2'] for name in model_names]\n",
    "bars = plt.bar(model_names, r2_scores, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'lightpink'])\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('R² Score Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight best model\n",
    "best_r2_idx = r2_scores.index(max(r2_scores))\n",
    "bars[best_r2_idx].set_color('darkgreen')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('d:/competition/air pollution/enhanced_model_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEnhanced model analysis complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final enhanced model performance summary:\")\n",
    "print(f\"- Best model: {best_model}\")\n",
    "print(f\"- Test RMSE: {test_results[best_model]['rmse']:.4f}\")\n",
    "print(f\"- Test R²: {test_results[best_model]['r2']:.4f}\")\n",
    "print(f\"- Features used: {len(selected_features)} out of {len(all_features)}\")\n",
    "print(f\"- Submission file: enhanced_submission.csv\")\n",
    "print(f\"- Analysis plots: enhanced_model_analysis.png\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
