{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35294350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TESTING MODE - Using minimal epochs for quick pipeline validation\n",
      "📊 Training Configuration: {'iterations': 10, 'n_estimators': 10, 'epochs': 1, 'n_trials': 2, 'cv_folds': 2, 'max_features': 20, 'early_stopping': 5}\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# CONFIGURATION: TESTING vs PRODUCTION\n",
    "# =====================================\n",
    "\n",
    "# Toggle between TESTING (fast, 1 epoch) and PRODUCTION (full training)\n",
    "TESTING_MODE = True  # Set to False for final production run\n",
    "\n",
    "if TESTING_MODE:\n",
    "    print(\"🧪 TESTING MODE - Using minimal epochs for quick pipeline validation\")\n",
    "    TRAINING_CONFIG = {\n",
    "        'iterations': 10,      # CatBoost iterations (minimum viable)\n",
    "        'n_estimators': 10,    # Tree-based models (minimum viable)\n",
    "        'epochs': 1,           # Neural networks\n",
    "        'n_trials': 2,         # Optuna optimization trials\n",
    "        'cv_folds': 2,         # Cross-validation folds\n",
    "        'max_features': 20,    # Reduced feature count for testing\n",
    "        'early_stopping': 5    # Early stopping rounds\n",
    "    }\n",
    "else:\n",
    "    print(\"🚀 PRODUCTION MODE - Full training with optimal parameters\")\n",
    "    TRAINING_CONFIG = {\n",
    "        'iterations': 500,     # CatBoost iterations  \n",
    "        'n_estimators': 500,   # Tree-based models\n",
    "        'epochs': 100,         # Neural networks\n",
    "        'n_trials': 100,       # Optuna optimization trials\n",
    "        'cv_folds': 5,         # Cross-validation folds\n",
    "        'max_features': 100,   # Full feature count\n",
    "        'early_stopping': 50   # Early stopping rounds\n",
    "    }\n",
    "\n",
    "print(f\"📊 Training Configuration: {TRAINING_CONFIG}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57d237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Cleaning old models and creating directories...\n",
      "✅ Directories cleaned and created\n",
      "🚀 ULTIMATE POLLUTION PREDICTION MODEL V5\n",
      "🔧 Enhanced with 100+ Features & BoxCox Transformation\n",
      "💾 Automatic Prediction Saving & Consistent Model Management\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# ULTIMATE POLLUTION PREDICTION MODEL V5\n",
    "# Enhanced with 100+ Features, BoxCox Transformation, and Consistent Model Management\n",
    "# =====================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import glob\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from scipy.stats import boxcox, yeojohnson\n",
    "import gc\n",
    "\n",
    "# Core ML Libraries\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, TimeSeriesSplit, KFold,\n",
    "    StratifiedKFold, GroupKFold\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    RobustScaler, StandardScaler, MinMaxScaler, PowerTransformer,\n",
    "    PolynomialFeatures, QuantileTransformer\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    explained_variance_score, median_absolute_error\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, f_regression, RFE, SelectFromModel,\n",
    "    mutual_info_regression\n",
    ")\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Advanced Models\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, ExtraTreesRegressor,\n",
    "    VotingRegressor, StackingRegressor, BaggingRegressor,\n",
    "    GradientBoostingRegressor, AdaBoostRegressor\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    Ridge, Lasso, ElasticNet, HuberRegressor,\n",
    "    QuantileRegressor, TheilSenRegressor, LinearRegression,\n",
    "    BayesianRidge, SGDRegressor\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Gradient Boosting Libraries\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# Advanced Analysis (if available)\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"SHAP not available - feature importance analysis will be limited\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Clean and create directories\n",
    "def clean_and_create_directories():\n",
    "    \"\"\"Remove old models and create fresh directories\"\"\"\n",
    "    directories = [\n",
    "        'models', 'models/individual', 'models/ensembles', 'models/robust',\n",
    "        'results', 'submissions', 'feature_analysis', 'transformations', 'predictions'\n",
    "    ]\n",
    "    \n",
    "    print(\"🧹 Cleaning old models and creating directories...\")\n",
    "    \n",
    "    # Remove old model files\n",
    "    for pattern in ['models/*.pkl', 'models/*/*.pkl', 'results/*.json', 'predictions/*.csv']:\n",
    "        files = glob.glob(pattern)\n",
    "        for file in files:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "                print(f\"   Removed: {file}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Create directories\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    print(\"✅ Directories cleaned and created\")\n",
    "\n",
    "clean_and_create_directories()\n",
    "\n",
    "print(\"🚀 ULTIMATE POLLUTION PREDICTION MODEL V5\")\n",
    "print(\"🔧 Enhanced with 100+ Features & BoxCox Transformation\")\n",
    "print(\"💾 Automatic Prediction Saving & Consistent Model Management\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =====================================\n",
    "# COMPREHENSIVE MODEL REGISTRY (Dynamic Configuration)\n",
    "# =====================================\n",
    "\n",
    "def create_model_registry(config):\n",
    "    \"\"\"Create model registry with dynamic configuration\"\"\"\n",
    "    return {\n",
    "        # Gradient Boosting Models\n",
    "        'catboost_mae': {\n",
    "            'class': cb.CatBoostRegressor,\n",
    "            'params': {\n",
    "                'loss_function': 'MAE',\n",
    "                'iterations': config['iterations'],\n",
    "                'depth': 6,\n",
    "                'learning_rate': 0.05,\n",
    "                'l2_leaf_reg': 3.0,\n",
    "                'bootstrap_type': 'Bayesian',\n",
    "                'bagging_temperature': 1.0,\n",
    "                'od_type': 'IncToDec',\n",
    "                'od_wait': config['early_stopping'],\n",
    "                'random_state': 42,\n",
    "                'verbose': False\n",
    "            }\n",
    "        },\n",
    "        'catboost_quantile': {\n",
    "            'class': cb.CatBoostRegressor,\n",
    "            'params': {\n",
    "                'loss_function': 'Quantile:alpha=0.5',\n",
    "                'iterations': config['iterations'],\n",
    "                'depth': 6,\n",
    "                'learning_rate': 0.05,\n",
    "                'l2_leaf_reg': 3.0,\n",
    "                'bootstrap_type': 'Bayesian',\n",
    "                'random_state': 42,\n",
    "                'verbose': False\n",
    "            }\n",
    "        },\n",
    "        'catboost_rmse': {\n",
    "            'class': cb.CatBoostRegressor,\n",
    "            'params': {\n",
    "                'loss_function': 'RMSE',\n",
    "                'iterations': config['iterations'],\n",
    "                'depth': 6,\n",
    "                'learning_rate': 0.05,\n",
    "                'l2_leaf_reg': 3.0,\n",
    "                'bootstrap_type': 'Bayesian',\n",
    "                'random_state': 42,\n",
    "                'verbose': False\n",
    "            }\n",
    "        },\n",
    "        'lgb_mae': {\n",
    "            'class': lgb.LGBMRegressor,\n",
    "            'params': {\n",
    "                'objective': 'mae',\n",
    "                'n_estimators': config['n_estimators'],\n",
    "                'learning_rate': 0.05,\n",
    "                'num_leaves': 31,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'random_state': 42,\n",
    "                'verbose': -1\n",
    "            }\n",
    "        },\n",
    "        'lgb_rmse': {\n",
    "            'class': lgb.LGBMRegressor,\n",
    "            'params': {\n",
    "                'objective': 'regression',\n",
    "                'n_estimators': config['n_estimators'],\n",
    "                'learning_rate': 0.05,\n",
    "                'num_leaves': 31,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'random_state': 42,\n",
    "                'verbose': -1\n",
    "            }\n",
    "        },\n",
    "        'xgb_mae': {\n",
    "            'class': xgb.XGBRegressor,\n",
    "            'params': {\n",
    "                'objective': 'reg:absoluteerror',\n",
    "                'n_estimators': config['n_estimators'],\n",
    "                'learning_rate': 0.05,\n",
    "                'max_depth': 6,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'random_state': 42\n",
    "            }\n",
    "        },\n",
    "        'xgb_rmse': {\n",
    "            'class': xgb.XGBRegressor,\n",
    "            'params': {\n",
    "                'objective': 'reg:squarederror',\n",
    "                'n_estimators': config['n_estimators'],\n",
    "                'learning_rate': 0.05,\n",
    "                'max_depth': 6,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'random_state': 42\n",
    "            }\n",
    "        },\n",
    "        'gradient_boost': {\n",
    "            'class': GradientBoostingRegressor,\n",
    "            'params': {\n",
    "                'n_estimators': config['n_estimators'] // 2,  # Adjust for slower training\n",
    "                'learning_rate': 0.1,\n",
    "                'max_depth': 6,\n",
    "                'subsample': 0.8,\n",
    "                'random_state': 42\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Tree-based Models\n",
    "        'rf_robust': {\n",
    "            'class': RandomForestRegressor,\n",
    "            'params': {\n",
    "                'n_estimators': config['n_estimators'],\n",
    "                'max_depth': 15,\n",
    "                'min_samples_split': 10,\n",
    "                'min_samples_leaf': 5,\n",
    "                'max_features': 'sqrt',\n",
    "                'bootstrap': True,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "        },\n",
    "        'extra_trees': {\n",
    "            'class': ExtraTreesRegressor,\n",
    "            'params': {\n",
    "                'n_estimators': config['n_estimators'],\n",
    "                'max_depth': 15,\n",
    "                'min_samples_split': 10,\n",
    "                'min_samples_leaf': 5,\n",
    "                'max_features': 'sqrt',\n",
    "                'bootstrap': True,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "        },\n",
    "        'bagging': {\n",
    "            'class': BaggingRegressor,\n",
    "            'params': {\n",
    "                'base_estimator': DecisionTreeRegressor(max_depth=10),\n",
    "                'n_estimators': config['n_estimators'] // 2,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "        },\n",
    "        'ada_boost': {\n",
    "            'class': AdaBoostRegressor,\n",
    "            'params': {\n",
    "                'n_estimators': config['n_estimators'] // 3,  # AdaBoost is slower\n",
    "                'learning_rate': 0.1,\n",
    "                'random_state': 42\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Linear Models\n",
    "        'ridge': {\n",
    "            'class': Ridge,\n",
    "            'params': {\n",
    "                'alpha': 1.0,\n",
    "                'random_state': 42\n",
    "            }\n",
    "        },\n",
    "        'lasso': {\n",
    "            'class': Lasso,\n",
    "            'params': {\n",
    "                'alpha': 0.1,\n",
    "                'random_state': 42,\n",
    "                'max_iter': 2000\n",
    "            }\n",
    "        },\n",
    "        'elastic_net': {\n",
    "            'class': ElasticNet,\n",
    "            'params': {\n",
    "                'alpha': 0.1,\n",
    "                'l1_ratio': 0.5,\n",
    "                'random_state': 42,\n",
    "                'max_iter': 2000\n",
    "            }\n",
    "        },\n",
    "        'huber': {\n",
    "            'class': HuberRegressor,\n",
    "            'params': {\n",
    "                'epsilon': 1.35,\n",
    "                'max_iter': 300\n",
    "            }\n",
    "        },\n",
    "        'theil_sen': {\n",
    "            'class': TheilSenRegressor,\n",
    "            'params': {\n",
    "                'random_state': 42,\n",
    "                'max_subpopulation': 1e4 if not TESTING_MODE else 1e3\n",
    "            }\n",
    "        },\n",
    "        'bayesian_ridge': {\n",
    "            'class': BayesianRidge,\n",
    "            'params': {\n",
    "                'compute_score': True\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Neural Network\n",
    "        'mlp_regressor': {\n",
    "            'class': MLPRegressor,\n",
    "            'params': {\n",
    "                'hidden_layer_sizes': (100, 50),\n",
    "                'max_iter': config['epochs'] * 10,  # MLPRegressor uses iterations\n",
    "                'random_state': 42,\n",
    "                'early_stopping': True,\n",
    "                'validation_fraction': 0.1,\n",
    "                'n_iter_no_change': config['early_stopping']\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Other Models\n",
    "        'knn': {\n",
    "            'class': KNeighborsRegressor,\n",
    "            'params': {\n",
    "                'n_neighbors': 5,\n",
    "                'weights': 'distance'\n",
    "            }\n",
    "        },\n",
    "        'svr_rbf': {\n",
    "            'class': SVR,\n",
    "            'params': {\n",
    "                'kernel': 'rbf',\n",
    "                'C': 1.0,\n",
    "                'gamma': 'scale'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Initialize MODEL_REGISTRY with current configuration\n",
    "# This will be updated when TRAINING_CONFIG is defined\n",
    "MODEL_REGISTRY = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e947a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# INITIALIZE MODEL REGISTRY & ENHANCED FEATURE ENGINEERING\n",
    "# =====================================\n",
    "\n",
    "# Initialize MODEL_REGISTRY with TRAINING_CONFIG\n",
    "MODEL_REGISTRY = create_model_registry(TRAINING_CONFIG)\n",
    "\n",
    "def print_model_summary():\n",
    "    \"\"\"Print a summary of all available models\"\"\"\n",
    "    print(\"\\n📋 COMPREHENSIVE MODEL REGISTRY SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    categories = {\n",
    "        'Gradient Boosting': ['catboost', 'lgb', 'xgb', 'gradient'],\n",
    "        'Tree-based': ['rf', 'extra', 'ada', 'decision', 'bagging'],\n",
    "        'Linear Models': ['ridge', 'lasso', 'elastic', 'huber', 'quantile', 'theil', 'bayesian', 'sgd', 'linear'],\n",
    "        'Other Models': ['knn', 'svr', 'mlp']\n",
    "    }\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        models = [name for name in MODEL_REGISTRY.keys() \n",
    "                 if any(keyword in name for keyword in keywords)]\n",
    "        if models:\n",
    "            print(f\"\\n{category}:\")\n",
    "            for model in models:\n",
    "                print(f\"  ✓ {model}\")\n",
    "    \n",
    "    print(f\"\\n📊 Total Models: {len(MODEL_REGISTRY)}\")\n",
    "    print(f\"🔧 Training Mode: {'TESTING' if TESTING_MODE else 'PRODUCTION'}\")\n",
    "\n",
    "print_model_summary()\n",
    "\n",
    "# =====================================\n",
    "# ENHANCED FEATURE ENGINEERING (100+ Features with BoxCox)\n",
    "# =====================================\n",
    "\n",
    "def apply_boxcox_transform(data, target_col=None):\n",
    "    \"\"\"Enhanced BoxCox transformation with stability improvements\"\"\"\n",
    "    transformed_data = data.copy()\n",
    "    \n",
    "    if target_col and target_col in transformed_data.columns:\n",
    "        target = transformed_data[target_col]\n",
    "        feature_cols = [col for col in transformed_data.columns if col != target_col]\n",
    "    else:\n",
    "        feature_cols = transformed_data.select_dtypes(include=[np.number]).columns\n",
    "        target = None\n",
    "    \n",
    "    print(f\"🔄 Applying BoxCox transformation to {len(feature_cols)} features...\")\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        try:\n",
    "            values = transformed_data[col].values\n",
    "            \n",
    "            # Skip if all values are the same\n",
    "            if np.std(values) < 1e-8:\n",
    "                continue\n",
    "                \n",
    "            # Make values positive by adding constant if needed\n",
    "            min_val = np.min(values)\n",
    "            if min_val <= 0:\n",
    "                values = values - min_val + 1\n",
    "            \n",
    "            # Apply BoxCox transformation\n",
    "            transformed_values, _ = boxcox(values)\n",
    "            transformed_data[col] = transformed_values\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If BoxCox fails, try Yeo-Johnson transformation\n",
    "            try:\n",
    "                transformed_values, _ = yeojohnson(transformed_data[col].values)\n",
    "                transformed_data[col] = transformed_values\n",
    "            except:\n",
    "                # If both fail, apply log transform\n",
    "                try:\n",
    "                    values = transformed_data[col].values\n",
    "                    if np.min(values) <= 0:\n",
    "                        values = values - np.min(values) + 1\n",
    "                    transformed_data[col] = np.log1p(values)\n",
    "                except:\n",
    "                    # Keep original if all transformations fail\n",
    "                    pass\n",
    "    \n",
    "    print(\"✅ BoxCox transformation completed!\")\n",
    "    return transformed_data\n",
    "\n",
    "def create_ultimate_features(data, is_train=True, max_features=None):\n",
    "    \"\"\"Create 100+ features with enhanced engineering\"\"\"\n",
    "    \n",
    "    if max_features is None:\n",
    "        max_features = TRAINING_CONFIG['max_features']\n",
    "    \n",
    "    print(f\"\\n\udd27 Creating ultimate features (target: {max_features})...\")\n",
    "    \n",
    "    # Start with original features\n",
    "    df = data.copy()\n",
    "    initial_features = len(df.columns)\n",
    "    \n",
    "    # Remove target if present for feature engineering\n",
    "    target_col = 'Pollution_value' if 'Pollution_value' in df.columns else None\n",
    "    if target_col:\n",
    "        target = df[target_col].copy()\n",
    "        df = df.drop(columns=[target_col])\n",
    "        print(f\"Target column preserved for feature engineering\")\n",
    "    else:\n",
    "        target = None\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"📊 Starting with {len(numeric_cols)} numeric features\")\n",
    "    \n",
    "    # 1. Statistical Features\n",
    "    print(\"\udd22 Creating statistical features...\")\n",
    "    for col in numeric_cols[:min(10, len(numeric_cols))]:  # Limit for testing mode\n",
    "        if col in df.columns:\n",
    "            # Rolling statistics (if time series structure exists)\n",
    "            try:\n",
    "                df[f'{col}_rolling_mean_3'] = df[col].rolling(window=3, min_periods=1).mean()\n",
    "                df[f'{col}_rolling_std_3'] = df[col].rolling(window=3, min_periods=1).std()\n",
    "                df[f'{col}_rolling_max_5'] = df[col].rolling(window=5, min_periods=1).max()\n",
    "                df[f'{col}_rolling_min_5'] = df[col].rolling(window=5, min_periods=1).min()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Lag features\n",
    "            try:\n",
    "                df[f'{col}_lag_1'] = df[col].shift(1)\n",
    "                df[f'{col}_lag_2'] = df[col].shift(2)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Mathematical transformations\n",
    "            if np.min(df[col]) > 0:\n",
    "                df[f'{col}_log'] = np.log1p(df[col])\n",
    "                df[f'{col}_sqrt'] = np.sqrt(df[col])\n",
    "                df[f'{col}_inv'] = 1 / (df[col] + 1e-8)\n",
    "            \n",
    "            # Squared and cubed features\n",
    "            df[f'{col}_squared'] = df[col] ** 2\n",
    "            df[f'{col}_cubed'] = df[col] ** 3\n",
    "    \n",
    "    # 2. Interaction Features\n",
    "    print(\"🔗 Creating interaction features...\")\n",
    "    feature_limit = min(8, len(numeric_cols))  # Limit combinations for testing\n",
    "    for i, col1 in enumerate(numeric_cols[:feature_limit]):\n",
    "        for j, col2 in enumerate(numeric_cols[i+1:feature_limit]):\n",
    "            if col1 in df.columns and col2 in df.columns:\n",
    "                try:\n",
    "                    # Basic interactions\n",
    "                    df[f'{col1}_mult_{col2}'] = df[col1] * df[col2]\n",
    "                    df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-8)\n",
    "                    df[f'{col1}_add_{col2}'] = df[col1] + df[col2]\n",
    "                    df[f'{col1}_sub_{col2}'] = df[col1] - df[col2]\n",
    "                    \n",
    "                    # Ratio features\n",
    "                    df[f'{col1}_ratio_{col2}'] = df[col1] / (df[col1] + df[col2] + 1e-8)\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # 3. Polynomial Features (limited selection)\n",
    "    print(\"📈 Creating polynomial features...\")\n",
    "    try:\n",
    "        selected_numeric = numeric_cols[:min(5, len(numeric_cols))]  # Limit for testing\n",
    "        if len(selected_numeric) >= 2:\n",
    "            poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "            poly_features = poly.fit_transform(df[selected_numeric].fillna(0))\n",
    "            poly_names = [f\"poly_{i}\" for i in range(poly_features.shape[1] - len(selected_numeric))]\n",
    "            poly_df = pd.DataFrame(poly_features[:, len(selected_numeric):], \n",
    "                                 columns=poly_names, index=df.index)\n",
    "            df = pd.concat([df, poly_df], axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Polynomial features creation failed: {e}\")\n",
    "    \n",
    "    # 4. Clustering Features\n",
    "    print(\"🎯 Creating clustering features...\")\n",
    "    try:\n",
    "        clustering_features = df[numeric_cols[:min(6, len(numeric_cols))]].fillna(0)\n",
    "        if len(clustering_features.columns) >= 2:\n",
    "            # K-means clustering\n",
    "            for n_clusters in [3, 5, 8]:\n",
    "                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "                df[f'cluster_{n_clusters}'] = kmeans.fit_predict(clustering_features)\n",
    "                \n",
    "                # Distance to cluster centers\n",
    "                centers = kmeans.cluster_centers_\n",
    "                for i in range(n_clusters):\n",
    "                    distances = np.sqrt(np.sum((clustering_features.values - centers[i])**2, axis=1))\n",
    "                    df[f'dist_to_cluster_{n_clusters}_{i}'] = distances\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Clustering features creation failed: {e}\")\n",
    "    \n",
    "    # 5. Time-based Features (if applicable)\n",
    "    print(\"⏰ Creating time-based features...\")\n",
    "    try:\n",
    "        # Create synthetic time features if no datetime column exists\n",
    "        df['synthetic_time'] = np.arange(len(df))\n",
    "        df['synthetic_time_sin'] = np.sin(2 * np.pi * df['synthetic_time'] / 24)\n",
    "        df['synthetic_time_cos'] = np.cos(2 * np.pi * df['synthetic_time'] / 24)\n",
    "        df['synthetic_trend'] = np.arange(len(df)) / len(df)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 6. Statistical Aggregations\n",
    "    print(\"📊 Creating aggregation features...\")\n",
    "    try:\n",
    "        # Row-wise statistics\n",
    "        numeric_subset = df.select_dtypes(include=[np.number])\n",
    "        df['row_mean'] = numeric_subset.mean(axis=1)\n",
    "        df['row_std'] = numeric_subset.std(axis=1)\n",
    "        df['row_max'] = numeric_subset.max(axis=1)\n",
    "        df['row_min'] = numeric_subset.min(axis=1)\n",
    "        df['row_median'] = numeric_subset.median(axis=1)\n",
    "        df['row_skew'] = numeric_subset.skew(axis=1)\n",
    "        df['row_kurt'] = numeric_subset.kurtosis(axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Apply BoxCox transformation\n",
    "    if target_col:\n",
    "        df = apply_boxcox_transform(df)\n",
    "    else:\n",
    "        df = apply_boxcox_transform(df)\n",
    "    \n",
    "    # Handle infinite and NaN values\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.fillna(df.median().fillna(0))\n",
    "    \n",
    "    # Feature selection to meet target count\n",
    "    current_features = len(df.columns)\n",
    "    print(f\"📊 Generated {current_features} features\")\n",
    "    \n",
    "    if current_features > max_features:\n",
    "        print(f\"🎯 Selecting top {max_features} features...\")\n",
    "        # Simple variance-based selection for now\n",
    "        variances = df.var().sort_values(ascending=False)\n",
    "        selected_features = variances.head(max_features).index.tolist()\n",
    "        df = df[selected_features]\n",
    "    \n",
    "    # Restore target column if it existed\n",
    "    if target_col and target is not None:\n",
    "        df[target_col] = target\n",
    "        print(f\"Target column restored after feature engineering\")\n",
    "    \n",
    "    final_features = len(df.columns) - (1 if target_col else 0)\n",
    "    print(f\"✅ Ultimate feature engineering completed!\")\n",
    "    print(f\"\udcca Final feature count: {final_features}\")\n",
    "    print(f\"📈 Feature increase: {final_features - initial_features}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_predictions(predictions, model_name, timestamp):\n",
    "    \"\"\"Save predictions in a consistent format\"\"\"\n",
    "    predictions_dir = 'predictions'\n",
    "    os.makedirs(predictions_dir, exist_ok=True)\n",
    "    \n",
    "    # Create predictions dataframe\n",
    "    pred_df = pd.DataFrame({\n",
    "        'id': range(len(predictions)),\n",
    "        'predicted_pollution': predictions\n",
    "    })\n",
    "    \n",
    "    # Save with consistent naming\n",
    "    filename = f\"{predictions_dir}/{model_name}_{timestamp}_predictions.csv\"\n",
    "    pred_df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"💾 Predictions saved: {filename}\")\n",
    "    return filename\n",
    "\n",
    "print(\"✅ Enhanced feature engineering functions ready!\")\n",
    "print(f\"🎯 Target features: {TRAINING_CONFIG['max_features']}\")\n",
    "print(\"🧪 BoxCox transformation enabled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc219b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# MAIN EXECUTION PIPELINE\n",
    "# =====================================\n",
    "\n",
    "def train_and_evaluate_models(X_train, X_val, y_train, y_val, timestamp):\n",
    "    \"\"\"Train all models and return results\"\"\"\n",
    "    model_results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    print(f\"\\n🚀 Training {len(MODEL_REGISTRY)} models...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for model_name, model_config in MODEL_REGISTRY.items():\n",
    "        try:\n",
    "            print(f\"\\n🔧 Training {model_name}...\")\n",
    "            \n",
    "            # Initialize model\n",
    "            model = model_config['class'](**model_config['params'])\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            train_pred = model.predict(X_train)\n",
    "            val_pred = model.predict(X_val)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            train_mae = mean_absolute_error(y_train, train_pred)\n",
    "            val_mae = mean_absolute_error(y_val, val_pred)\n",
    "            train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "            val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "            train_r2 = r2_score(y_train, train_pred)\n",
    "            val_r2 = r2_score(y_val, val_pred)\n",
    "            \n",
    "            # Store results\n",
    "            model_results[model_name] = {\n",
    "                'train_mae': train_mae,\n",
    "                'val_mae': val_mae,\n",
    "                'train_rmse': train_rmse,\n",
    "                'val_rmse': val_rmse,\n",
    "                'train_r2': train_r2,\n",
    "                'val_r2': val_r2,\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "            trained_models[model_name] = model\n",
    "            \n",
    "            # Save model\n",
    "            model_path = f\"models/individual/{model_name}_{timestamp}.pkl\"\n",
    "            joblib.dump(model, model_path)\n",
    "            \n",
    "            print(f\"    ✅ VAL MAE: {val_mae:.4f} | VAL RMSE: {val_rmse:.4f} | VAL R²: {val_r2:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ❌ Failed to train {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return model_results, trained_models\n",
    "\n",
    "def create_ensemble_models(trained_models, X_train, X_val, y_train, y_val, timestamp):\n",
    "    \"\"\"Create ensemble models from trained individual models\"\"\"\n",
    "    print(f\"\\n🎯 Creating ensemble models...\")\n",
    "    \n",
    "    ensemble_results = {}\n",
    "    \n",
    "    if len(trained_models) < 2:\n",
    "        print(\"⚠️ Not enough models for ensemble creation\")\n",
    "        return ensemble_results\n",
    "    \n",
    "    try:\n",
    "        # Voting Regressor\n",
    "        print(\"🗳️ Creating Voting Regressor...\")\n",
    "        voting_models = list(trained_models.items())[:min(5, len(trained_models))]  # Limit for testing\n",
    "        voting_regressor = VotingRegressor(voting_models)\n",
    "        voting_regressor.fit(X_train, y_train)\n",
    "        \n",
    "        val_pred = voting_regressor.predict(X_val)\n",
    "        val_mae = mean_absolute_error(y_val, val_pred)\n",
    "        \n",
    "        ensemble_results['voting_regressor'] = {\n",
    "            'val_mae': val_mae,\n",
    "            'model': voting_regressor\n",
    "        }\n",
    "        \n",
    "        # Save ensemble model\n",
    "        ensemble_path = f\"models/ensembles/voting_regressor_{timestamp}.pkl\"\n",
    "        joblib.dump(voting_regressor, ensemble_path)\n",
    "        \n",
    "        print(f\"    ✅ Voting Regressor VAL MAE: {val_mae:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ❌ Voting Regressor failed: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Stacking Regressor (if we have enough models)\n",
    "        if len(trained_models) >= 3:\n",
    "            print(\"📚 Creating Stacking Regressor...\")\n",
    "            base_models = list(trained_models.items())[:3]  # Use top 3 models\n",
    "            meta_model = Ridge(random_state=42)\n",
    "            \n",
    "            stacking_regressor = StackingRegressor(\n",
    "                estimators=base_models,\n",
    "                final_estimator=meta_model,\n",
    "                cv=TRAINING_CONFIG['cv_folds']\n",
    "            )\n",
    "            stacking_regressor.fit(X_train, y_train)\n",
    "            \n",
    "            val_pred = stacking_regressor.predict(X_val)\n",
    "            val_mae = mean_absolute_error(y_val, val_pred)\n",
    "            \n",
    "            ensemble_results['stacking_regressor'] = {\n",
    "                'val_mae': val_mae,\n",
    "                'model': stacking_regressor\n",
    "            }\n",
    "            \n",
    "            # Save ensemble model\n",
    "            ensemble_path = f\"models/ensembles/stacking_regressor_{timestamp}.pkl\"\n",
    "            joblib.dump(stacking_regressor, ensemble_path)\n",
    "            \n",
    "            print(f\"    ✅ Stacking Regressor VAL MAE: {val_mae:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    ❌ Stacking Regressor failed: {e}\")\n",
    "    \n",
    "    return ensemble_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline with testing/production mode support\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🚀 ULTIMATE POLLUTION PREDICTION PIPELINE\")\n",
    "    print(f\"🧪 Mode: {'TESTING' if TESTING_MODE else 'PRODUCTION'}\")\n",
    "    print(f\"🎯 Target Features: {TRAINING_CONFIG['max_features']}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create timestamp for consistent file naming\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load Data\n",
    "        print(\"\\n📂 Loading data...\")\n",
    "        train_data = pd.read_csv('train.csv')\n",
    "        test_data = pd.read_csv('test.csv')\n",
    "        \n",
    "        print(f\"    ✅ Train data: {train_data.shape}\")\n",
    "        print(f\"    ✅ Test data: {test_data.shape}\")\n",
    "        \n",
    "        # 2. Feature Engineering with configurable features\n",
    "        print(f\"\\n🔧 Feature engineering with {TRAINING_CONFIG['max_features']} features...\")\n",
    "        train_engineered = create_ultimate_features(train_data, is_train=True)\n",
    "        test_engineered = create_ultimate_features(test_data, is_train=False)\n",
    "        \n",
    "        # Ensure consistent columns\n",
    "        common_features = list(set(train_engineered.columns) & set(test_engineered.columns))\n",
    "        if 'Pollution_value' in common_features:\n",
    "            common_features.remove('Pollution_value')\n",
    "        \n",
    "        print(f\"    ✅ Common features: {len(common_features)}\")\n",
    "        \n",
    "        # 3. Prepare training data\n",
    "        print(\"\\n🎯 Preparing training data...\")\n",
    "        X = train_engineered[common_features]\n",
    "        y = train_engineered['Pollution_value']\n",
    "        X_test = test_engineered[common_features]\n",
    "        \n",
    "        # Train-validation split\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"    ✅ Training set: {X_train.shape}\")\n",
    "        print(f\"    ✅ Validation set: {X_val.shape}\")\n",
    "        print(f\"    ✅ Test set: {X_test.shape}\")\n",
    "        \n",
    "        # 4. Scale features\n",
    "        print(\"\\n⚖️ Scaling features...\")\n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Convert back to DataFrames\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "        X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "        \n",
    "        # Save scaler\n",
    "        scaler_path = f\"transformations/scaler_{timestamp}.pkl\"\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        print(f\"    ✅ Scaler saved: {scaler_path}\")\n",
    "        \n",
    "        # 5. Train individual models\n",
    "        model_results, trained_models = train_and_evaluate_models(\n",
    "            X_train_scaled, X_val_scaled, y_train, y_val, timestamp\n",
    "        )\n",
    "        \n",
    "        # 6. Create ensemble models\n",
    "        ensemble_results = create_ensemble_models(\n",
    "            trained_models, X_train_scaled, X_val_scaled, y_train, y_val, timestamp\n",
    "        )\n",
    "        \n",
    "        # 7. Select best model and make predictions\n",
    "        print(f\"\\n🏆 Selecting best model...\")\n",
    "        all_results = {**model_results, **ensemble_results}\n",
    "        \n",
    "        if all_results:\n",
    "            best_model_name = min(all_results.keys(), key=lambda x: all_results[x]['val_mae'])\n",
    "            best_model = all_results[best_model_name]['model']\n",
    "            best_mae = all_results[best_model_name]['val_mae']\n",
    "            \n",
    "            print(f\"    🏆 Best Model: {best_model_name}\")\n",
    "            print(f\"    📊 Best VAL MAE: {best_mae:.4f}\")\n",
    "            \n",
    "            # Make final predictions\n",
    "            print(f\"\\n🔮 Making final predictions...\")\n",
    "            final_predictions = best_model.predict(X_test_scaled)\n",
    "            \n",
    "            # Save predictions\n",
    "            pred_filename = save_predictions(final_predictions, best_model_name, timestamp)\n",
    "            \n",
    "            # Create submission file\n",
    "            submission_df = pd.DataFrame({\n",
    "                'id': range(len(final_predictions)),\n",
    "                'Pollution_value': final_predictions\n",
    "            })\n",
    "            \n",
    "            submission_path = f\"submissions/submission_{best_model_name}_{timestamp}.csv\"\n",
    "            submission_df.to_csv(submission_path, index=False)\n",
    "            print(f\"📤 Submission saved: {submission_path}\")\n",
    "            \n",
    "            # Save results summary\n",
    "            results_summary = {\n",
    "                'timestamp': timestamp,\n",
    "                'mode': 'TESTING' if TESTING_MODE else 'PRODUCTION',\n",
    "                'best_model': best_model_name,\n",
    "                'best_val_mae': float(best_mae),\n",
    "                'n_features': len(common_features),\n",
    "                'training_config': TRAINING_CONFIG,\n",
    "                'model_results': {k: {metric: float(v) for metric, v in model_results[k].items() \n",
    "                                     if metric != 'model'} for k in model_results.keys()},\n",
    "                'ensemble_results': {k: {metric: float(v) for metric, v in ensemble_results[k].items() \n",
    "                                        if metric != 'model'} for k in ensemble_results.keys()}\n",
    "            }\n",
    "            \n",
    "            results_path = f\"results/results_summary_{timestamp}.json\"\n",
    "            with open(results_path, 'w') as f:\n",
    "                json.dump(results_summary, f, indent=2)\n",
    "            \n",
    "            print(f\"📊 Results summary saved: {results_path}\")\n",
    "            \n",
    "            # Print final summary\n",
    "            print(f\"\\n\" + \"=\" * 80)\n",
    "            print(f\"🎉 PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "            print(f\"🏆 Best Model: {best_model_name}\")\n",
    "            print(f\"📊 Validation MAE: {best_mae:.4f}\")\n",
    "            print(f\"🔧 Features Used: {len(common_features)}\")\n",
    "            print(f\"💾 Predictions: {pred_filename}\")\n",
    "            print(f\"📤 Submission: {submission_path}\")\n",
    "            print(f\"🧪 Mode: {'TESTING' if TESTING_MODE else 'PRODUCTION'}\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ No models were successfully trained!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Pipeline failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# =====================================\n",
    "# MANUAL EXECUTION SECTION\n",
    "# =====================================\n",
    "\n",
    "print(\"\\n🎯 PIPELINE READY!\")\n",
    "print(\"📝 To run the pipeline, call: main()\")\n",
    "print(\"🧪 Current mode:\", \"TESTING\" if TESTING_MODE else \"PRODUCTION\")\n",
    "print(\"⚙️ To change mode, modify TESTING_MODE in the first cell\")\n",
    "print(\"\\n💡 Example usage:\")\n",
    "print(\"   main()  # Run the complete pipeline\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f1cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1433657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
